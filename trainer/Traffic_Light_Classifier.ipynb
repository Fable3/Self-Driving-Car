{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "## Traffic Light Recognition Classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "\n",
    "training_file = \"train.p\"\n",
    "\n",
    "#csv_files=['daySequence1', 'daySequence2']\n",
    "csv_files=['daySequence1',\n",
    "           'daySequence2',\n",
    "           'dayTrain/dayClip1',\n",
    "           'dayTrain/dayClip2',\n",
    "           'dayTrain/dayClip3',\n",
    "           'dayTrain/dayClip4',\n",
    "           'dayTrain/dayClip5',\n",
    "           'dayTrain/dayClip6',\n",
    "           'dayTrain/dayClip7',\n",
    "           'dayTrain/dayClip8',\n",
    "           'dayTrain/dayClip9',\n",
    "           'dayTrain/dayClip10',\n",
    "           'dayTrain/dayClip11',\n",
    "           'dayTrain/dayClip12',\n",
    "           'dayTrain/dayClip13']\n",
    "\n",
    "path_prefix='lisa-traffic-light-dataset/Annotations/Annotations/'\n",
    "path_postfix='/frameAnnotationsBOX.csv'\n",
    "\n",
    "def frame_name(pre, seq):\n",
    "    sp = pre.split('/')\n",
    "    if (len(sp)==1):\n",
    "        return 'lisa-traffic-light-dataset/'+pre+'/'+pre+'/frames/'+pre+('--%05d.jpg'%seq)\n",
    "    else:\n",
    "        return 'lisa-traffic-light-dataset/'+sp[0]+'/'+sp[0]+'/'+sp[1]+'/frames/'+sp[1]+('--%05d.jpg'%seq)\n",
    "\n",
    "\n",
    "#samples = []\n",
    "#reload_images = True\n",
    "reload_images = False\n",
    "if reload_images:\n",
    "    images=[]\n",
    "    labels=[]\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        with open(path_prefix+csv_file+path_postfix, 'r') as f:\n",
    "            reader = csv.reader(f, delimiter=';')\n",
    "            next(reader) # skip header\n",
    "            count=0\n",
    "            for row in reader:\n",
    "                annot = [csv_file]\n",
    "                annot.append(row[7])\n",
    "                annot.extend(row[1:6])\n",
    "                #print (frame_name(annot[0], int(annot[1])))\n",
    "                #print(annot)\n",
    "                im = Image.open(frame_name(annot[0], int(annot[1])))\n",
    "                image_pos=(int(annot[3]), int(annot[4]), int(annot[5]), int(annot[6]))\n",
    "                im = im.crop(image_pos)\n",
    "                x=image_pos[2]-image_pos[0]\n",
    "                y=image_pos[3]-image_pos[1]\n",
    "                dest_image = Image.new('RGB', (32, 32))\n",
    "                dest_image.paste((0,0,0), (0, 0, 32, 32))\n",
    "                if (x>32 or y>32):\n",
    "                    if (x>y):\n",
    "                        im=im.resize((32, y*32//x), Image.BILINEAR)\n",
    "                    else:\n",
    "                        im=im.resize((x*32//y, 32), Image.BILINEAR)\n",
    "\n",
    "                img_w, img_h = im.size\n",
    "                dest_image.paste(im, ((32-img_w)//2, (32-img_h)//2) )\n",
    "                arr=np.asarray(dest_image)\n",
    "                strlbl = annot[2]\n",
    "                label=-1\n",
    "                if (strlbl=='stop'):label=0\n",
    "                elif (strlbl=='stopLeft'):label=0\n",
    "                elif (strlbl=='warning'):label=1\n",
    "                elif (strlbl=='warningLeft'):label=1\n",
    "                elif (strlbl=='go'):label=2\n",
    "                elif (strlbl=='goLeft'):label=2\n",
    "                elif (strlbl=='goForward'):label=2\n",
    "                else: print(\"invalid label:\"+strlbl)\n",
    "                if (label!=-1):\n",
    "                    images.append(arr)\n",
    "                    labels.append(label)\n",
    "\n",
    "                count+=1\n",
    "                if (count%500==0):\n",
    "                    print(csv_file, ' frame ', annot[1], ' label ', str(label))\n",
    "                    plt.rcParams['figure.figsize'] = [1, 1]\n",
    "                    plt.imshow(arr)\n",
    "                    plt.show()\n",
    "\n",
    "                #if (count>110): break\n",
    "    images = np.asarray(images)\n",
    "    labels = np.asarray(labels)\n",
    "    train={\"images\":images, \"labels\":labels}\n",
    "    with open(training_file, mode='wb') as f:\n",
    "        pickle.dump(train, f)\n",
    "else:\n",
    "    with open(training_file, mode='rb') as f:\n",
    "        train = pickle.load(f)\n",
    "        images=train[\"images\"]\n",
    "        labels=train[\"labels\"]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(images, labels, test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Dataset Summary & Exploration\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide a Basic Summary of the Data Set Using Python, Numpy and/or Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples = 44664\n",
      "Number of testing examples = 11166\n",
      "Number of testing examples = 0\n",
      "Image data shape = (32, 32)\n",
      "Number of classes = 3\n"
     ]
    }
   ],
   "source": [
    "### Replace each question mark with the appropriate value. \n",
    "### Use python, pandas or numpy methods rather than hard coding the results\n",
    "\n",
    "# TODO: Number of training examples\n",
    "n_train = X_train.shape[0]\n",
    "\n",
    "# TODO: Number of validation examples\n",
    "n_validation = X_valid.shape[0]\n",
    "\n",
    "# TODO: Number of testing examples.\n",
    "n_test = 0 #test[\"features\"].shape[0]\n",
    "\n",
    "# TODO: What's the shape of an traffic sign image?\n",
    "image_shape = (X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "# TODO: How many unique classes/labels there are in the dataset.\n",
    "n_classes = max(y_train)+1\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_validation)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include an exploratory visualization of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the German Traffic Signs Dataset using the pickled file(s). This is open ended, suggestions include: plotting traffic sign images, plotting the count of each sign, etc. \n",
    "\n",
    "The [Matplotlib](http://matplotlib.org/) [examples](http://matplotlib.org/examples/index.html) and [gallery](http://matplotlib.org/gallery.html) pages are a great resource for doing visualizations in Python.\n",
    "\n",
    "**NOTE:** It's recommended you start with something simple first. If you wish to do more, come back to it after you've completed the rest of the sections. It can be interesting to look at the distribution of classes in the training, validation and test set. Is the distribution the same? Are there more examples of some classes than others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Data exploration visualization code goes here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Visualizations will be shown in the notebook.\n",
    "%matplotlib inline\n",
    "\n",
    "fig=plt.figure(figsize=(10, 10))\n",
    "columns = 8\n",
    "rows = int((n_classes-1)/columns+1)\n",
    "\n",
    "for i in range(columns*rows):\n",
    "    indices=[]\n",
    "    for idx in range(len(y_train)):\n",
    "           if y_train[idx]==i: indices.append(idx)\n",
    "    \n",
    "    if (len(indices)!=0):\n",
    "        selected_index = indices[random.randint(0, len(indices))]\n",
    "        image = X_train[selected_index].squeeze()\n",
    "\n",
    "        fig.add_subplot(rows, columns, i+1)\n",
    "        plt.title(\"#{} ({})\".format(i, len(indices)))\n",
    "        ax = plt.gca()\n",
    "        ax.axes.xaxis.set_visible(False)\n",
    "        ax.axes.yaxis.set_visible(False)\n",
    "        plt.imshow(image)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Step 2: Design and Test a Model Architecture\n",
    "\n",
    "Design and implement a deep learning model that learns to recognize traffic signs. Train and test your model on the [German Traffic Sign Dataset](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset).\n",
    "\n",
    "The LeNet-5 implementation shown in the [classroom](https://classroom.udacity.com/nanodegrees/nd013/parts/fbf77062-5703-404e-b60c-95b78b2f3f9e/modules/6df7ae49-c61c-4bb2-a23e-6527e69209ec/lessons/601ae704-1035-4287-8b11-e2c2716217ad/concepts/d4aca031-508f-4e0b-b493-e7b706120f81) at the end of the CNN lesson is a solid starting point. You'll have to change the number of classes and possibly the preprocessing, but aside from that it's plug and play! \n",
    "\n",
    "With the LeNet-5 solution from the lecture, you should expect a validation set accuracy of about 0.89. To meet specifications, the validation set accuracy will need to be at least 0.93. It is possible to get an even higher accuracy, but 0.93 is the minimum for a successful project submission. \n",
    "\n",
    "There are various aspects to consider when thinking about this problem:\n",
    "\n",
    "- Neural network architecture (is the network over or underfitting?)\n",
    "- Play around preprocessing techniques (normalization, rgb to grayscale, etc)\n",
    "- Number of examples per label (some have more than others).\n",
    "- Generate fake data.\n",
    "\n",
    "Here is an example of a [published baseline model on this problem](http://yann.lecun.com/exdb/publis/pdf/sermanet-ijcnn-11.pdf). It's not required to be familiar with the approach used in the paper but, it's good practice to try to read papers like these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the Data Set (normalization, grayscale, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimally, the image data should be normalized so that the data has mean zero and equal variance. For image data, `(pixel - 128)/ 128` is a quick way to approximately normalize the data and can be used in this project. \n",
    "\n",
    "Other pre-processing steps are optional. You can try different techniques to see if it improves performance. \n",
    "\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocess the data here. It is required to normalize the data. Other preprocessing steps could include \n",
    "### converting to grayscale, etc.\n",
    "### Feel free to use as many code cells as needed.\n",
    "import cv2\n",
    "import matplotlib.image as mpimg\n",
    "import skimage.transform\n",
    "\n",
    "def RandomRotate(image):\n",
    "    #img=img.copy()\n",
    "    rnd=random.uniform(-10, 10)\n",
    "    rnd+=np.sign(rnd)*5 # -15..-5 or 5..15 degree range\n",
    "    #return img.copy()\n",
    "    #print(\"rotate\"+str(rnd))\n",
    "    res=skimage.transform.rotate(image, rnd, mode='edge')\n",
    "    return res\n",
    "\n",
    "def modify_images(X_data):\n",
    "    arr=X_data.copy()\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4,4))\n",
    "    for idx in range(len(arr)):\n",
    "        arr[idx] = cv2.cvtColor(arr[idx], cv2.COLOR_RGB2LAB)\n",
    "        l_clahe = clahe.apply(arr[idx,:,:,0])\n",
    "        arr[idx,:,:,0] = l_clahe\n",
    "        m = np.max(arr[idx,:,:,0])\n",
    "        if (m>1):\n",
    "            arr[idx,:,:,0]=np.uint8(np.float32(arr[idx,:,:,0])*255/m)\n",
    "        arr[idx] = cv2.cvtColor(arr[idx], cv2.COLOR_LAB2RGB)\n",
    "    return (np.float32(arr[:,:,:,:])-128)/128\n",
    "    #for idx in range(len(X_data)):\n",
    "        #arr[idx] = cv2.cvtColor(X_data[idx], cv2.COLOR_RGB2HSV)\n",
    "        #arr[idx,:, :, 0] = cv2.cvtColor(X_data[idx], cv2.COLOR_RGB2GRAY)\n",
    "    #pure layer: return arr[:,:,:,1:2]\n",
    "    #return (np.float32(arr[:,:,:,0:1])-128)/128\n",
    "    #for hue: 0..180, and green is at 60, shift it to be at 0 instead of red, which jumps from 0 to 179\n",
    "    #return (np.float32(((arr[:,:,:,0:1]-60)%180)-90)/90)\n",
    "    \n",
    "#X_test_mod =modify_images(X_test)\n",
    "def EnsureMinimalClassCount(X, y):\n",
    "    y_max = max(y)\n",
    "    min_class_count = int(len(y)/(y_max+1)/2)\n",
    "    print(\"minimal samples per class: {}\".format(min_class_count))\n",
    "    img_count=0\n",
    "    for i in range(y_max+1):\n",
    "        class_count = len(y[y==i])\n",
    "        #indices=[]\n",
    "        #for idx in range(len(y)):\n",
    "        #    if y[idx]==i: indices.append(idx)\n",
    "        if (class_count!=0 and class_count<min_class_count):\n",
    "            duplication_num = int((min_class_count-1)/class_count)\n",
    "            print(\"class {} count {} adding {} duplicates\".format(i, class_count, duplication_num))\n",
    "            source_X=X[y==i]\n",
    "            source_y=y[y==i]\n",
    "            for d in range(duplication_num):\n",
    "                new_X = np.copy(source_X)\n",
    "                for r in range(len(source_y)):\n",
    "                    #new_X[r]=RandomRotate(source_X[r])\n",
    "                    #mpimg.imsave('debug/rot{}_source.png'.format(img_count), source_X[r])\n",
    "                    rnd=random.uniform(-10, 10)\n",
    "                    rnd+=np.sign(rnd)*5 # -15..-5 or 5..15 degree range\n",
    "                    res=skimage.transform.rotate(source_X[r], rnd, mode='edge')\n",
    "                    new_X[r]=np.uint8(res*255)\n",
    "                    #mpimg.imsave('debug/rot{}.png'.format(img_count), new_X[r])\n",
    "                    #img_count+=1\n",
    "                X = np.concatenate((X, new_X), axis=0)\n",
    "                y = np.concatenate((y, source_y), axis=0)\n",
    "    return X, y\n",
    "\n",
    "#X_train, y_train = EnsureMinimalClassCount(X_train, y_train)\n",
    "\n",
    "X_train_mod = modify_images(X_train)\n",
    "X_valid_mod = modify_images(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fable\\miniconda3\\envs\\IntroToTensorFlow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:458: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Fable\\miniconda3\\envs\\IntroToTensorFlow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:459: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Fable\\miniconda3\\envs\\IntroToTensorFlow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:460: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Fable\\miniconda3\\envs\\IntroToTensorFlow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:461: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Fable\\miniconda3\\envs\\IntroToTensorFlow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:462: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Fable\\miniconda3\\envs\\IntroToTensorFlow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:465: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "### Define your architecture here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "displayed_activation1 = None\n",
    "displayed_activation2 = None\n",
    "\n",
    "def LeNet(x):    \n",
    "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    global displayed_activation1\n",
    "    global displayed_activation2\n",
    "    \n",
    "    # Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6.\n",
    "    F_W = tf.Variable(tf.truncated_normal([5, 5, 3, 6], mean=mu, stddev = sigma))\n",
    "    F_b = tf.Variable(tf.zeros([6]))\n",
    "    layer1 = tf.nn.conv2d(x, F_W, [1, 1, 1, 1], 'VALID') + F_b\n",
    "\n",
    "    #  Activation.\n",
    "    layer1 = tf.nn.relu(layer1)\n",
    "    displayed_activation1 = layer1\n",
    "\n",
    "    # Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "    layer1pool =tf.nn.max_pool(layer1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # Layer 2: Convolutional. Output = 10x10x16.\n",
    "    F_W2 = tf.Variable(tf.truncated_normal([5, 5, 6, 16], mean=mu, stddev = sigma))\n",
    "    F_b2 = tf.Variable(tf.Variable(tf.zeros([16])))\n",
    "    layer2 = tf.nn.conv2d(layer1pool, F_W2, [1,1,1,1],'VALID') + F_b2\n",
    "    \n",
    "    # Activation.\n",
    "    layer2 = tf.nn.relu(layer2)\n",
    "    displayed_activation2 = layer2\n",
    "\n",
    "    # Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "    layer2pool =tf.nn.max_pool(layer2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # Flatten. Input = 5x5x16. Output = 400.\n",
    "    layer2flat = tf.contrib.layers.flatten(layer2pool)\n",
    "    \n",
    "    # Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "    F_W3 = tf.Variable(tf.truncated_normal([400, 120], mean=mu, stddev = sigma))\n",
    "    F_b3 = tf.Variable(tf.Variable(tf.zeros([120])))\n",
    "    \n",
    "    layer3 = tf.matmul(layer2flat, F_W3)\n",
    "    layer3 = tf.nn.bias_add(layer3, F_b3)\n",
    "    \n",
    "    # Activation.\n",
    "    layer3 = tf.nn.relu(layer3)\n",
    "    layer3 = tf.nn.dropout(layer3, drop_keep)\n",
    "\n",
    "    # Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    F_W4 = tf.Variable(tf.truncated_normal([120, 84], mean=mu, stddev = sigma))\n",
    "    F_b4 = tf.Variable(tf.Variable(tf.zeros([84])))\n",
    "    \n",
    "    layer4 = tf.matmul(layer3, F_W4)\n",
    "    layer4 = tf.nn.bias_add(layer4, F_b4)\n",
    "    \n",
    "    # Activation.\n",
    "    layer4 = tf.nn.relu(layer4)\n",
    "    layer4 = tf.nn.dropout(layer4, drop_keep)\n",
    "\n",
    "    # Layer 5: Fully Connected. Input = 84. Output = n_classes.\n",
    "    F_W5 = tf.Variable(tf.truncated_normal([84, n_classes], mean=mu, stddev = sigma))\n",
    "    F_b5 = tf.Variable(tf.Variable(tf.zeros([n_classes])))\n",
    "    \n",
    "    layer5 = tf.matmul(layer4, F_W5)\n",
    "    layer5 = tf.nn.bias_add(layer5, F_b5, name='result')\n",
    "    \n",
    "    return layer5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Validate and Test the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A validation set can be used to assess how well the model is performing. A low accuracy on the training and validation\n",
    "sets imply underfitting. A high accuracy on the training set but low accuracy on the validation set implies overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "EPOCH 1 ...\n",
      "Validation Accuracy = 0.999\n",
      "\n",
      "EPOCH 2 ...\n",
      "Validation Accuracy = 0.999\n",
      "\n",
      "EPOCH 3 ...\n",
      "Validation Accuracy = 0.999\n",
      "\n",
      "EPOCH 4 ...\n",
      "Validation Accuracy = 1.000\n",
      "\n",
      "EPOCH 5 ...\n",
      "Validation Accuracy = 0.999\n",
      "\n",
      "{'epocs': 5, 'batch_size': 128, 'rate': 0.001, 'acc': [0.999, 0.999, 0.999, 1.000, 0.999], 'tra': []}\n",
      "restoring from epoch 3 accuracy 1.000\n",
      "INFO:tensorflow:Restoring parameters from ./early_stop_checkpoint\n",
      "Model saved\n",
      "INFO:tensorflow:Froze 36 variables.\n",
      "Converted 36 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "### Train your model here.\n",
    "### Calculate and report the accuracy on the training and validation set.\n",
    "### Once a final model architecture is selected, \n",
    "### the accuracy on the test set should be calculated and reported as well.\n",
    "### Feel free to use as many code cells as needed.\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 3), name=\"input\")\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "drop_keep = tf.placeholder(tf.float32)\n",
    "one_hot_y = tf.one_hot(y, n_classes)\n",
    "rate = 0.001\n",
    "\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "logits = LeNet(x)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)\n",
    "\n",
    "optimizer_slow = tf.train.AdamOptimizer(learning_rate = rate/10)\n",
    "training_operation_slow = optimizer.minimize(loss_operation)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y, drop_keep: 1.0})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples\n",
    "\n",
    "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
    "    \"\"\"\n",
    "    Freezes the state of a session into a pruned computation graph.\n",
    "\n",
    "    Creates a new computation graph where variable nodes are replaced by\n",
    "    constants taking their current value in the session. The new graph will be\n",
    "    pruned so subgraphs that are not necessary to compute the requested\n",
    "    outputs are removed.\n",
    "    @param session The TensorFlow session to be frozen.\n",
    "    @param keep_var_names A list of variable names that should not be frozen,\n",
    "                          or None to freeze all the variables in the graph.\n",
    "    @param output_names Names of the relevant graph outputs.\n",
    "    @param clear_devices Remove the device directives from the graph for better portability.\n",
    "    @return The frozen graph definition.\n",
    "    \"\"\"\n",
    "    graph = session.graph\n",
    "    with graph.as_default():\n",
    "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "        output_names += [v.op.name for v in tf.global_variables()]\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = \"\"\n",
    "        frozen_graph = tf.graph_util.convert_variables_to_constants(\n",
    "            session, input_graph_def, output_names, freeze_var_names)\n",
    "        return frozen_graph\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train_mod)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    class prettyfloat(float):\n",
    "        def __repr__(self):\n",
    "            return \"%0.3f\" % self\n",
    "    stats={\"epocs\":EPOCHS, \"batch_size\":BATCH_SIZE, \"rate\":rate, \"acc\":[], \"tra\":[]}\n",
    "    best_accuracy = 0\n",
    "    best_epoch = 0\n",
    "    need_restore = False\n",
    "    checkpoint_path='./early_stop_checkpoint'\n",
    "    for i in range(EPOCHS):\n",
    "        X_train_mod, y_train = shuffle(X_train_mod, y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train_mod[offset:end], y_train[offset:end]\n",
    "            if (i>EPOCHS/2):\n",
    "                sess.run(training_operation_slow, feed_dict={x: batch_x, y: batch_y, drop_keep: .5})\n",
    "            else:\n",
    "                sess.run(training_operation, feed_dict={x: batch_x, y: batch_y, drop_keep: .5})\n",
    "            \n",
    "        validation_accuracy = evaluate(X_valid_mod, y_valid)\n",
    "        if (validation_accuracy>best_accuracy):\n",
    "            best_accuracy = validation_accuracy\n",
    "            best_epoch = i\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            need_restore=False\n",
    "        else:\n",
    "            need_restore=True\n",
    "        \n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        calculate_training_accuracy=False\n",
    "        if (calculate_training_accuracy):\n",
    "            training_accuracy = evaluate(X_train_mod, y_train)\n",
    "            print(\"Validation Accuracy = {:.3f} Training Accuracy = {:.3f}\".format(validation_accuracy, training_accuracy))\n",
    "            stats[\"tra\"].append(prettyfloat(training_accuracy))\n",
    "        else:\n",
    "            print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        stats[\"acc\"].append(prettyfloat(validation_accuracy))\n",
    "        \n",
    "        print()\n",
    "    print(str(stats))\n",
    "    if (need_restore):\n",
    "        print(\"restoring from epoch {} accuracy {:.3f}\".format(best_epoch, best_accuracy))\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "        \n",
    "    saver.save(sess, './lenet')\n",
    "    print(\"Model saved\")\n",
    "    \n",
    "    frozen_graph = freeze_session(sess, output_names=[\"result\"])\n",
    "\n",
    "    tf.train.write_graph(frozen_graph, 'model', 'fablenet.pb', as_text=False)\n",
    "    #test_accuracy = evaluate(modify_images(X_test), y_test)\n",
    "    #print(\"Testing Accuracy = {:.3f}\".format(test_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
