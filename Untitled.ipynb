{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Driving Car Capstone Project\n",
    "The goal of this project is to use ROS code to integrate with Carla, Udacity's Self Driving Car. To get there our team utilized a simulator that works very similarly to Carla to drive on a simulated highway with traffic lights. The project then transitions into the real world and is tested on Carla in a closed track to mimick various conditions similar to a real public road environment.\n",
    "\n",
    "[//]: # (Image References)\n",
    "\n",
    "[image1]: /img/final-project-ros-graph-v2.png \"ROS System\"\n",
    "\n",
    "### Team\n",
    "| Name | Email |\n",
    "| :------------------------------- |:------------------------------------|\n",
    "| Krishan Patel (Team Leader)  | kspatel95@gmail.com |\n",
    "| Tamás Kerecsen                    | kerecsen@gmail.com |\n",
    "| Tomoya Matsumura               | tomoya.matsumura@gmail.com |\n",
    "| György Blahut                        | fable3@gmail.com |\n",
    "\n",
    "### To get Ros code started (Simulator)\n",
    "```\n",
    "1. cd Self-Driving-Car\n",
    "2. ./run.sh\n",
    "```\n",
    "### If setup is needed for downloading required packages\n",
    "``` \n",
    "./setup.sh\n",
    "```\n",
    "---\n",
    "\n",
    "## ROS System\n",
    "\n",
    "The ROS system utilized in the Simulator is intended to be modeled very similarly to the Udacity Self Driving Car. \n",
    "The architecture of the ROS nodes was defined in the project repository, but\n",
    "some of the modules have not (or not fully) been implemented.\n",
    "Our task was to develop these missing portions, including the modules listed below.  \n",
    "\n",
    "![Final score][image1]\n",
    "\n",
    "### Main Components of the project\n",
    "---\n",
    "#### Waypoint Updater\n",
    "\n",
    "The simulated car as well as the self driving car are provided with \n",
    "waypoints that our software receives through the ROS System. The Waypoint Updater \n",
    "node is in charge of building a route with these waypoints, and continuously transferring\n",
    "the next points to tell the car where to go. \n",
    "At startup, the car is given base waypoints for the entire route. \n",
    "Our code only looks at a portion of the waypoints that is immediately relevant to the car. \n",
    "It starts by finding the closest waypoint ahead of the car and \n",
    "LOOKAHEAD_WPS number of waypoints further ahead, then makes any adjustments \n",
    "needed based on traffic lights or obstacles.  \n",
    "If obstacles or traffic lights are detected, a path is planned that slows\n",
    "down ahead of the stop line or obstacle. \n",
    "Deceleration concept is as following picture. Once the vehicle detected the next traffic light as red, the target velocities for each waypoint are defined as linear deceleration till the stopline waypoint except the below condition.\n",
    "- the calculated velocity is lower than CONST_LOWSPEED, set CONST_LOWSPEED as target speed\n",
    "- distance to stopline waypoint within CONST_LOWSPEED_RANGE, set CONST_LOWSPEED as target speed.\n",
    "- distance to stopline waypoint within ABS_STOP_RANGE, set SQRT deceleration to absolutely stop before the stopline.\n",
    "![alt text](writeup_illustrations/deceleration.png \"Deceleration concept\")\n",
    "\n",
    "#### Traffic Light Detection\n",
    "\n",
    "The TLDetector node is responsible for receiving the camera image from an upward\n",
    "facing camera, and identifying if there are red or green lights ahead of the car.\n",
    "The node also receives a list of the traffic light locations and the corresponding \n",
    "stop lines, which makes this task a bit easier.\n",
    "If a red light is identified, the node publishes a `traffic_waypoint` message,\n",
    "informing other nodes (primarily the Waypoint Updater) of the potential\n",
    "obstacle ahead.\n",
    "\n",
    "The TLDetector uses a neural network to identify the traffic lights and their\n",
    "color on the camera image. Selecting the type of neural network, and training it\n",
    " for the particular requirements of the project turned out to be a bit of a\n",
    "  challenge. There are two very different scenarios where the visual\n",
    "   classifier network has to function. One is the simulator, and the other\n",
    "    one is the real world\n",
    "test track, where Carla runs. The appearance of the background and the\n",
    " traffic light is very different in the two scenarios, so either two separate\n",
    "  classifiers have to be developed for the two scenarios, or the one has to\n",
    "   be very flexible with regards to the appearance of the lights and the\n",
    "    background.  \n",
    "    \n",
    "In addition to this challenge, Carla runs a very old Tensorflow 1.3 and\n",
    " Python 2.7 environment, that rules out many off-the-shelf solutions\n",
    "available in modern Keras and Python environments.\n",
    "\n",
    "We tried the approaches below with more or less success to find the best compromise between\n",
    "performance, recognition quality in the simulator and on the ROSbag recorded images\n",
    " from the test track, as well as compatibility with the environment.\n",
    " \n",
    "##### Whole-image classifier\n",
    "The first approach we tried was the one we perfected in the Behavioral\n",
    " Cloning section of the course: training a recognizer on entire pictures and\n",
    " hoping it learns how to distinguish the traffic light colors automatically. \n",
    " We applied what we learned about Transfer Learning, took the MobileNet V1\n",
    " 224x224 classifier (the only one available in this old version of Keras as\n",
    "  a turn-key base classifier) and tried to train it on images captured from\n",
    "   the simulator.\n",
    "\n",
    "But first we needed an annotated image database that would allow training or\n",
    "hand-tuning the recognizers. So we created a tool  (`src/trafficlight_capturer`) \n",
    "that runs as a ROS node, and combines the gound\n",
    "truth information coming from the simulator in the `vehicle/traffic_lights`\n",
    "ROS topic with the images coming from the simulator in the `/image_color`\n",
    "topic and built a database of annotated images: images combined the traffic\n",
    "light state. \n",
    "\n",
    "When traffic lights are calculated to be visible (and close enough) based on the\n",
    " pose of the car and that of the traffic light, the tool saves each incoming\n",
    "  image to a folder structure that is compatible with the Keras\n",
    "   ImageDataGenerator (using the traffic light state helpfully provided by the simulator).\n",
    "\n",
    "A trainer python tool (`trainer/trainer.py`) takes these images and\n",
    " retrains the MobileNet V1 sample in Keras with the images as input.\n",
    "\n",
    "The resulting classifier worked acceptably well, once it was close enough, it\n",
    " recognized the traffic light colors, but it was noisy and unpredictable when\n",
    "  farther away from the lights. It obviously didn't work for the real-world\n",
    "   scenario at all without further training. \n",
    "   \n",
    "Although we probably could have fulfilled the project rubric with this\n",
    " classifier, we decided to move on an try to find a more robust and universal solution.\n",
    " \n",
    "##### SSD-style classifer + secondary classifier\n",
    "The next obvious way to solve the problem of traffic light detection is to\n",
    "find an SSD-style one-shot classifier that is capable of finding the traffic\n",
    "lights in the image, and combine it with a secondary CV-based or neural\n",
    "network based subclassifier to identify the traffic light state (color). \n",
    "The secondary classifier is necessary because off-the-shelf pre-trained SSD\n",
    "classifiers are typically trained on the Coco database, and that database\n",
    " doesn't label red, green and yellow light separately, they are all just\n",
    "  labeled as \"traffic lights\".\n",
    "\n",
    "Based on the literature, there are two obvious candidates for an SSD style\n",
    " classifier that can run at a high framerate in the constrained hardware\n",
    "  environment of a car. One is MobileNet SSD, and the other is Yolo. There\n",
    "   are multiple configurations available for each, and we selected\n",
    "MobileNet SSD V1 224 with training data from the Mobile Zoo as one candidate\n",
    ", and YoloV3-tiny-pnr as another one.  \n",
    "\n",
    "First, we integrated MobileNet SSD into the ROS environment. We included it\n",
    " in the trafficlight_capture node, and outputted the detection results\n",
    "  (colored rectangles overlaid on the camera image) into a new, `/image_recognized`\n",
    "   ROS topic. This way we could observe how the model  was behaving using at\n",
    "    real time with the help of `rqt_image_viewer`\n",
    " \n",
    "After lowering the threshold detection confidence to 20%, the MobileNet SSD\n",
    " network was finding most of the traffic lights in the simulator, but it\n",
    "  also found a lot of side-facing and back-facing traffic lights as well. \n",
    "  It also didn't do a whole lot of recognizing with the real-world images from the ROSbag. \n",
    "\n",
    "So after some research we identified the Yolo family as another potentially\n",
    " acceptable trade-off between speed and quality. The full-blown YoloV4 was\n",
    "  too slow at least in the VM environment, but there was a very promising, \n",
    "  recently developed candidate, YoloV3-tiny-PRN, which has a very small 16MB parameter file and\n",
    "    runs at 10+ fps even in CPU-only mode. (see https://github.com/WongKinYiu\n",
    "    /PartialResidualNetworks and https://github.com/AlexeyAB/darknet).\n",
    "     \n",
    "Unfortunately Yolo natively runs under the darknet neural network development\n",
    " tool, not Tensorflow. There are Tensorflow ports of Yolo, but we didn't\n",
    "  manage to get any of them working with the published weight files (we\n",
    "   backported a couple to python 2.7 and tensorflow 1.3, but for some reason\n",
    "   the way they were loading the weights\n",
    "   - which involves a lot of bit shuffling and tensorflow surgery - failed in\n",
    "    our environment). It would have been possible to train such a model from\n",
    "     scratch, but it would have required more training hours than what we had\n",
    "      available in the workspace.\n",
    "      \n",
    "So we looked at pulling in darknet, the original execution environment of\n",
    " Yolo, as a module into the python/ROS architecture we were building. We\n",
    "  managed to get a darknet `.so` library built with `catkin-make`, and call the\n",
    "   library from our python software, and were able to deeply test this option\n",
    "    as well.  It worked better than MobileNet SSD did, but had the same\n",
    "     essential shortcomings (extraneous bounding boxes on rear-facing and\n",
    "      side-facing traffic lights, missed traffic lights, major difficulties\n",
    "       with the parking lot scenario).\n",
    "\n",
    "The secondary classifier works on small images, once the primary classifier identified the box of a traffic light.\n",
    "There are 2 possible solutions, either try to use CV tools to classify, or train a separate neural network.\n",
    "Since the problem at hand was very similar to the Traffic Sign Classifier project, we trained a LeNet network to do the job.\n",
    "To prepare for real-life scenarios, we trained the network on 55,000 images from the LISA Traffic Light Dataset.\n",
    "The trainer code is in trainer/Traffic_Light_Classifier.ipynb, the combination of MobileNet and the secondary network is in trainer/Combined.ipynb\n",
    "\n",
    "While this is a very strong combination in real world situations, in the parking lot MobileNet couldn't reliably detect the traffic light, and when it did, the\n",
    "secondary classifier had problems. Green light was classified yellow or red, due to the heavy yellow color from the frame.\n",
    "\n",
    "There are 3 ROSbag files which we used for testing, using the first word of the filename they're called Just, Loop and Train.\n",
    "The first 2 was promising, but the 3rd one had problems even on the pretrained MobileNet part.\n",
    "Here's an annotated video of the \"just_traffic_light\" ROSbag: [fablenet_just.mp4](writeup_illustrations/fablenet_just.mp4)\n",
    "     \n",
    "Both MobileNet and Yolo, even in their maxed-out configurations reacted very-very\n",
    " poorly to the yellow colored traffic light used in the parking lot\n",
    " testing with Carla. Apparently the Coco dataset doesn't have yellow traffic\n",
    "  lights, therefore the recognizers trained on this dataset have no clue what\n",
    "   such a yellow object might be. This situation called for custom training...\n",
    "\n",
    "##### Custom-trained SSD-style classifier \n",
    "The final approach we tried was to train an MobileNet SSD network on our\n",
    "specific yellow real-world traffic light, and our simulated, but \n",
    "traditional-looking lights. And since we had to start from scratch, we decided to\n",
    "separately train for each color, and get traffic light states with a one-shot \n",
    "process. So the hope was that we would end up with a classifier that can do 3\n",
    " different positive predictions (red, yellow and green light positions) and\n",
    "  one negative prediction (no traffic lights visible).\n",
    "\n",
    "First we checked if we could take advantage of transfer learning, and could\n",
    " re-train a full-blown MobileNet SSD model to our special requirements. This \n",
    "approach remained elusive, as it would have needed several days of training\n",
    " time on a high-end GPU.\n",
    "\n",
    "Fortunately during this process we found this fantastic github repository: \n",
    "https://github.com/pierluigiferrari/ssd_keras \n",
    "Among other things, he describes a miniature \"SSD7\" architecture, that is\n",
    " trainable within the means of regular developers. It takes a 2-3 hours to fully\n",
    "  train from scratch in the workspace with the K80 GPU, and 5-10 times that on\n",
    "   a home PC, which is still reasonable.\n",
    "   \n",
    "Here is the architecture of the SSD7 network:\n",
    "![alt text](writeup_illustrations/model_1.png \"SSD7 architecture\")\n",
    "   \n",
    "The next challenge was to obtain an augmented dataset which includes color\n",
    " information and bounding boxes for our two sets of traffic lights. We\n",
    "  managed to generate some of this data using semi-automated methods (see\n",
    "   details below), but we\n",
    "   also had to do several hours of hand-labeling. Altogether we accumulated\n",
    "  over 1600 labeled images, which was sufficient to get fairly robust training\n",
    "   results. The performance of the model never reached 100%, but when\n",
    "    averaging 2 out of 3 detections the results were practically perfect.   \n",
    " \n",
    "The semi-automated annotation tool is in trainer/AutoAnnot.ipynb.\n",
    "It uses the manually edited annot_hint.txt file, and the images from the training ROSbag file.\n",
    "The numbers in the text file are the frame numbers, r/g/y is the color, which is the same for subsequent images, so only captured at changes.\n",
    "The tool tries to extract a bounding box, but sometimes only half the traffic light is identified, or extra boxes would appear.\n",
    "The text file is from manually checking all the images for complete boxes, and recording their numbers.\n",
    "When there's no box at all, the image is skipped automatically, which helps entering ranges.\n",
    "The detection algorithm on the LAB color model, which is good for identifing yellow. It's enhanched with CLAHE.\n",
    "Sobel finds vertical edges. After tresholding, HoughTransformP finds the lines.\n",
    "Lines are filtered so that they need to be mostly vertical, in pairs, and the resulting box width has to be 0.3 to 0.6 times the height.\n",
    "The result can be seen in annotated video form: [traffic_lights_training_annot.mp4](writeup_illustrations/traffic_lights_training_annot1.mp4)\n",
    "\n",
    "Here are some examples of how the network behaves on different datasets:\n",
    "\n",
    "<div>\n",
    "    <video width=\"99%\" height=\"640\" autoplay loop muted>\n",
    "        <source src=\"writeup_illustrations/ssd7_fablemagic.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "</div>\n",
    "\n",
    "[Parking lot](writeup_illustrations/ssd7_fablemagic.mp4) and\n",
    "[Simulator](writeup_illustrations/ssd7_simulation.mp4)\n",
    "\n",
    "There are 2 separate topics for image: image_color and image_raw.\n",
    "The simulator only sends image_color, the Just and the Loop ROSbag files only had image_raw, the Train ROSbag had both.\n",
    "The Traffic Light Detector subscribes for both topics. If both topics have messages at the same time, image_color has priority.\n",
    "Ihe image_raw uses Bayer-encoded black and white image, which is converted to color using OpenCV.\n",
    "\n",
    "To obtain the images from the ROSbag files, the messages are converted to text:\n",
    "```\n",
    "rostopic echo /image_raw -b traffic_light_training.bag > training_image_raw\n",
    "```\n",
    "The resulting text file is converted to RGB image list with the `trainer/raw2img.py` script.\n",
    "\n",
    "Training was based on the distorted camera images, so undistorting images before prediction would be detrimental.\n",
    "\n",
    "To confirm that traffic light detection works on real life images, we used the ROSbag playback while the simulator was running as well.\n",
    "The car was \"parked\" before a traffic light in the simulator by manually crashing it into a tree. This was necessary to trigger the detection.\n",
    "Camera feed was switched off in the simulator, the tl_detector node was working on the ROSbag images.\n",
    "\n",
    "#### Drive By Wire Node\n",
    "\n",
    "The DBW Node uses the data coming from the waypoint updater to know when to apply different commands to throttle, braking, and steering. By default the car is going to throttle till it hits the speed limit or the vehicle limit, as the waypoints require some steering or braking the car relies on updates from the twist_controller.py. The Twist Controller  utilizes a combination of PID, a Low Pass Filter, and a Yaw Controller all to have a smoother and more effective response to control the car. The Twist Controller ensures that the car is able to make a full stop when needed, steer to stay within the lanes, and maintain speed when possible which feeds directly into the DBW Node when enabled.\n",
    "\n",
    "---\n",
    "\n",
    "### Udacity Capstone Project README\n",
    "\n",
    "This is the project repo for the final project of the Udacity Self-Driving Car Nanodegree: Programming a Real Self-Driving Car. For more information about the project, see the project introduction [here](https://classroom.udacity.com/nanodegrees/nd013/parts/6047fe34-d93c-4f50-8336-b70ef10cb4b2/modules/e1a23b06-329a-4684-a717-ad476f0d8dff/lessons/462c933d-9f24-42d3-8bdc-a08a5fc866e4/concepts/5ab4b122-83e6-436d-850f-9f4d26627fd9).\n",
    "\n",
    "Please use **one** of the two installation options, either native **or** docker installation.\n",
    "\n",
    "### Native Installation\n",
    "\n",
    "* Be sure that your workstation is running Ubuntu 16.04 Xenial Xerus or Ubuntu 14.04 Trusty Tahir. [Ubuntu downloads can be found here](https://www.ubuntu.com/download/desktop).\n",
    "* If using a Virtual Machine to install Ubuntu, use the following configuration as minimum:\n",
    "  * 2 CPU\n",
    "  * 2 GB system memory\n",
    "  * 25 GB of free hard drive space\n",
    "\n",
    "  The Udacity provided virtual machine has ROS and Dataspeed DBW already installed, so you can skip the next two steps if you are using this.\n",
    "\n",
    "* Follow these instructions to install ROS\n",
    "  * [ROS Kinetic](http://wiki.ros.org/kinetic/Installation/Ubuntu) if you have Ubuntu 16.04.\n",
    "  * [ROS Indigo](http://wiki.ros.org/indigo/Installation/Ubuntu) if you have Ubuntu 14.04.\n",
    "* [Dataspeed DBW](https://bitbucket.org/DataspeedInc/dbw_mkz_ros)\n",
    "  * Use this option to install the SDK on a workstation that already has ROS installed: [One Line SDK Install (binary)](https://bitbucket.org/DataspeedInc/dbw_mkz_ros/src/81e63fcc335d7b64139d7482017d6a97b405e250/ROS_SETUP.md?fileviewer=file-view-default)\n",
    "* Download the [Udacity Simulator](https://github.com/udacity/CarND-Capstone/releases).\n",
    "\n",
    "### Docker Installation\n",
    "[Install Docker](https://docs.docker.com/engine/installation/)\n",
    "\n",
    "Build the docker container\n",
    "```bash\n",
    "docker build . -t capstone\n",
    "```\n",
    "\n",
    "Run the docker file\n",
    "```bash\n",
    "docker run -p 4567:4567 -v $PWD:/capstone -v /tmp/log:/root/.ros/ --rm -it capstone\n",
    "```\n",
    "\n",
    "### Port Forwarding\n",
    "To set up port forwarding, please refer to the \"uWebSocketIO Starter Guide\" found in the classroom (see Extended Kalman Filter Project lesson).\n",
    "\n",
    "### Usage\n",
    "\n",
    "1. Clone the project repository\n",
    "```bash\n",
    "git clone https://github.com/udacity/CarND-Capstone.git\n",
    "```\n",
    "\n",
    "2. Install python dependencies\n",
    "```bash\n",
    "cd CarND-Capstone\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "3. Make and run styx\n",
    "```bash\n",
    "cd ros\n",
    "catkin_make\n",
    "source devel/setup.sh\n",
    "roslaunch launch/styx.launch\n",
    "```\n",
    "4. Run the simulator\n",
    "\n",
    "### Real world testing\n",
    "1. Download [training bag](https://s3-us-west-1.amazonaws.com/udacity-selfdrivingcar/traffic_light_bag_file.zip) that was recorded on the Udacity self-driving car.\n",
    "2. Unzip the file\n",
    "```bash\n",
    "unzip traffic_light_bag_file.zip\n",
    "```\n",
    "3. Play the bag file\n",
    "```bash\n",
    "rosbag play -l traffic_light_bag_file/traffic_light_training.bag\n",
    "```\n",
    "4. Launch your project in site mode\n",
    "```bash\n",
    "cd CarND-Capstone/ros\n",
    "roslaunch launch/site.launch\n",
    "```\n",
    "5. Confirm that traffic light detection works on real life images\n",
    "\n",
    "### Other library/driver information\n",
    "Outside of `requirements.txt`, here is information on other driver/library versions used in the simulator and Carla:\n",
    "\n",
    "Specific to these libraries, the simulator grader and Carla use the following:\n",
    "\n",
    "|        | Simulator | Carla  |\n",
    "| :-----------: |:-------------:| :-----:|\n",
    "| Nvidia driver | 384.130 | 384.130 |\n",
    "| CUDA | 8.0.61 | 8.0.61 |\n",
    "| cuDNN | 6.0.21 | 6.0.21 |\n",
    "| TensorRT | N/A | N/A |\n",
    "| OpenCV | 3.2.0-dev | 2.4.8 |\n",
    "| OpenMP | N/A | N/A |\n",
    "\n",
    "We are working on a fix to line up the OpenCV versions between the two.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
